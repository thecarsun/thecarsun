#systemsthinking 

`STATUS:` draft     

---

The common story about AI and job displacement is linear: AI arrives --> jobs disappear  --> people suffer. Reality, business, and economics are not linear. They are dynamic systems of feedback loops, delays, incentives and unintended consequences. Systems Thinking Framework helps to see topics or problem statements as a whole, not just headlines and echoes of headlines.

> ***The real risk isn't our job title. It's how we practice the role***

**Context**

Systems Thinking Framework is a method to help understand complex problems by looking at the whole system and how those parts interact over time (Not just isolated parts such as "AI layoff/take over/become  homeless"). Systems thinking asks us to think about: what feedback loops are created? What delays exist? What can we actually do, actions to take?

**PROBLEM STATEMENT** : AI displacing jobs/AI taking over people's jobs 

**SYSTEM BOUNDARY** 

- What problem are we actually solving?
	- *Definitely not by blowing up Skynet like how it was done in the Terminator Franchise.* Instead think along the lines of: *how does societies and individuals adapt and work with AI and its many use cases and tools?*

- Where does this system start and end?

    **Starts**: Business and economic shifts to automate to reduce cost, speed, and scale

	**Ends**: Dysfunctions of labor market equilibrium after widespread of AI adoption		
	*System Goal Question*: What are we optimizing for in this system today? Shareholder return, GDP growth? What is it? The system behaves according to its goal. If the goal is straight up cost efficiency then *displacement* in this context is not a bug - it is a feature. 

- What is inside vs. outside of our control?
	
	|  **Inside Control**  |  **Outside Control** | 
	| :--- | :--- |
	|Reskilling Investment |Pace of model capability improvements |
	|How we design human-AI workflows |Macroeconomic cycles |
	|Hiring criteria and role definitions |Geopolitical competition driving automation |
	|Education system reform |Rate of AI cost decline |
	|Regulatory frameworks |Global labor arbitrage |


**KEY COMPONENTS** 

*Systems are made of things that accumulate (**Stocks**) and rates of change (**Flows**)*.

**STOCKS**: *things that accumulate (talent pool, technical debt, public trust)*
- Talent pool (skilled workers available)
- Public trust in AI systems
- Technical debt in legacy systems
- Institutional knowledge (what walks out the door when people leave)
- Reskilling capacity (educators, programs, infrastructure)
- Political policies for intervention

**FLOWS**: *rates of change (hiring rate, layoffs, reskilling speed)*
- Hiring rate / layoff rate (low hire/low fire)
- Reskilling speed
- Rate of AI adoption by industry
- Rate of new job category creation
- Policy response rate

*Questions to think about:*
- What builds up slowly but impacts massively?
- What drains faster than it can be replenished?
- What has an invisible ceiling?

**RELATIONSHIP MAPPING** (Causal Loops)

*In the context of Systems Thinking Framework: 
Causal Loops sometimes = the same force that causes a problem to begin with often feeds back and makes the problem worse or eventually corrects itself (resolved/corrected).*

**REINFORCING LOOPS (R)** - *amplify change, create momentum*

**R1: The AI Spiral** AI integration at companies --> cost savings --> more investment in AI --> more automation --> more displacement --> more pressure to cut costs --> more AI investment

*This loop creates the economy --> Companies that don't automate/integrate AI  get outcompeted by ones that do. Thinking about in history, similar patterns took place with cloud migration waves, electrification, and mechanization, and more..*

**R2: The Talent Concentration Loop** AI tools increase the leverage of top performers --> organizations concentrate hiring at the top --> mid-tier talent loses opportunities --> skill gaps widen --> fewer people can participate in the AI economy

*The K economy in human capital.*

**R3: The Potential Reskilling Demand Loop** (This is conditional) Displacements --> demand for reskilling programs --> investment in training --> more capable workforce --> higher adaptability --> better outcomes

*This is positive, however only if policies are in place, buy-ins are achieved, and investments are in place. It won't start without deliberate intervention. Policies and enforcements also can take decades*

**BALANCING LOOPS (B)** - *resist change, create stability or corrections*

**B1: Political Pressure** Job displacement --> public anger/social unrest --> political pressure --> regulation --> slower AI adoption

*The question is whether it arrives before or after the damage is done. Most policy responses lag by years.**

**B2: Wage Correction** Labor surplus in displaced roles --> wages fall --> some automation becomes less economically attractive --> adoption slows in those niches

*Markets correct, but slow and uneven. Significant human cost throughout the correction period.*

**B3: New Job Creation** AI tools --> new areas/needs --> new roles emerge --> labor absorbs into new areas

*Historically, this happened with electrification and the internet.  The question is: how long will the in-between?*

**DELAYS**

*Delays create instability and is the dangerous part of the complex systems. When there are lags between cause and effect it leads to over or under correction behaviors.*

| **Domain** | **Delay** |
| :--- | :--- |
|Individual reskilling |1–3 years |
|Education system reform |5–10 years |
|Cultural shifts ("what jobs are valued") |One generation |
|Infrastructure upgrades |Quarters |
|Policy responses |Election cycles (2–6 years) |
|AI capability development |in acceleration mode now/on going |

*AI is accelerating, institutional systems and human adaptations do not match the speed of AI.
*AI capability cycles operate in months. Education reform cycles operate in decades.*
Think: Why did most systems fail in the past?*

**LEVERAGE POINTS**

*Leverage points rank from weakest to the most powerful - based on [**Donella Meadows**](https://systemsthinkingalliance.org/donella-meadows-pioneering-contributions-to-systems-thinking-and-environmental-advocacy/)'s work*

**WEAK**: 
- Adjust parameters such as more budget, more headcount, or cut the work force to budget needs
- Tweak the rules such as layoff packages, extending employment, hire contractors vs full-time employees

**MEDIUM:** 
- Change the feedback loops: make AI adoption _trigger_ automatic reskilling investment rather than being a separate decision
- Change information flows: real-time labor market data visible to workers, not just employers

**STRONGER**:
- Change system goals: instead of _"optimize AI efficiency,"_ make the goal _"optimize human + AI collaboration"_ — this changes what gets measured, funded, and rewarded
- Change who has power: give workers and communities a seat at the table when automation decisions are made

**POWERFUL**
- Change the paradigm: the mental model people operate from 
*Current dominant mental model** - Paradigm: AI is displacing jobs
*Reframe* - alternate Paradigm: Humans design systems that includes AI to create value
*Think: What should humans spend their time on that AI + machines can't?*

**WHAT AI IS AND ISN'T GOOD AT**
*Understand this area helps to define where the risks live*

**AI is excellent at**:
- Recognize patterns in large datasets
- Very well defined and structured tasks 
- High volume and repeatable processes
- Optimize within clear parameters

**AI is bad at**:
- Political ambiguity and competing incentives
- Human dynamics (trust, negotiation)
- Unclear ownership and messy accountability and its nuance
- Problems where the goal itself is unclear or disputed
- Situations with no prior data
- Ethical/moral judgements with real life consequences

**THE RISK**

*Back to the overarching theme*
> ***The real risk is not our job title, it is how we practice the role***

The TPM that functions in heavy coordination is at high risk. 
The TPM that is systems oriented (navigates the teams disconnect/gaps, anticipates constraints, and can orchestrate the different teams and integrates AI into that workflow is at lower risk. Same title, but different execution. 

The role that repeats their tasks is at high risk.
The role that can synthesize incomplete information, manages relationships, anticipates, actively design and manage the flow of their work and its impacts is at low risk. 

The pattern: High-risk roles operate inside of the AI's zone of competence. 
Low risk roles operate outside of it. 

The AI displacement debate often assumes a single inevitable outcome.
Systems thinking suggests something different:
Outcomes depend on which loops are strengthened,
which delays are shortened,
and which goals are prioritized.

--- 
*If you like this read and want to see more, please let me know! Feel free to reach out with any feedback/questions/suggestions. Thank you*